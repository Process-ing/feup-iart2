{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9444027e",
   "metadata": {},
   "source": [
    "# Backpack Prediction Challenge\n",
    "\n",
    "**FEUP 2024/2025 - L.EIC029 IART**\n",
    "\n",
    "- Bruno Oliveira - 202208700  \n",
    "- Henrique Fernandes - 202204988  \n",
    "- Rodrigo Coelho - 202205188  \n",
    "\n",
    "> Based on Kaggle Playground Season 5, Episode 2  \n",
    "> April 2025\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "\n",
    "### Virtual Environment\n",
    "\n",
    "In order to setup the project, use the following commands to setup a virtual environment and install the needed dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dfa6d",
   "metadata": {},
   "source": [
    "Once the dependencies are installed, the script below can be used to download the dataset from the Kaggle competition, using your Kaggle account.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Don't forget to download the Kaggle token associated with your account from the <a href=\"https://www.kaggle.com/settings\">Settings page</a>, move it to the current folder and join the <a href=\"https://www.kaggle.com/competitions/playground-series-s5e2/\">Kaggle playground competition</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa775246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "dataset_zip = data_dir / \"playground-series-s5e2.zip\"\n",
    "dataset_train = data_dir / \"train.csv\"\n",
    "dataset_extra = data_dir / \"training_extra.csv\"\n",
    "\n",
    "if not dataset_train.exists() or not dataset_extra.exists():\n",
    "    if not dataset_zip.exists():\n",
    "        print(\"Dataset zip not found. Downloading from Kaggle...\")\n",
    "        !kaggle competitions download -c playground-series-s5e2\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Dataset zip already exists.\")\n",
    "\n",
    "    print(\"Unzipping the dataset...\")\n",
    "    !unzip -o playground-series-s5e2.zip -d data\n",
    "    !rm playground-series-s5e2.zip\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download and extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999020a2",
   "metadata": {},
   "source": [
    "### Loading the Datasets\n",
    "\n",
    "With the dependencies met and having downloaded the dataset, we can now load it into our environment. The following commands wil load boat of the datasets:\n",
    "\n",
    "- `train.csv` which contains 300000 entries and is used to train the models\n",
    "- `test.csv` which contains 200000 entries and is used to test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(dataset_train)\n",
    "\n",
    "data.info()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1f71c",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be864bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame({\n",
    "    'Column': data.columns,\n",
    "    'Missing Train Values': data.isnull().sum().values,\n",
    "    'Percentage of Missing Train Values': data.isnull().sum().values / len(data) * 100\n",
    "})\n",
    "\n",
    "missing_values = missing_values[~missing_values['Column'].isin(['id', 'Price'])]\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259fcdd",
   "metadata": {},
   "source": [
    "### Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_duplicates = data.drop('id', axis=1).duplicated().sum()\n",
    "print(f\"Data duplicates: {data_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e83d67",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1794664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id', axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddadd5",
   "metadata": {},
   "source": [
    "### Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "categorical_columns = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "numerical_columns = ['Compartments', 'Weight Capacity (kg)', 'Price']\n",
    "\n",
    "FIGURE_WIDTH = 18\n",
    "PLOTS_PER_ROW = 3\n",
    "\n",
    "def plot_categorical_columns(df):\n",
    "    num_columns = len(categorical_columns)\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, column in enumerate(categorical_columns):\n",
    "        sns.countplot(df[column], order=df[column].value_counts().index, ax=axes[i])\n",
    "        axes[i].set_title(f\"Distribution of {column}\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_numeric_columns(df):\n",
    "    num_columns = len(numerical_columns)\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, column in enumerate(numerical_columns):\n",
    "        sns.histplot(df[column], ax=axes[i], discrete=column == 'Compartments')\n",
    "        axes[i].set_title(f\"Distribution of {column}\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_columns(data)\n",
    "plot_numeric_columns(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4893593",
   "metadata": {},
   "source": [
    "### Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5082b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "FIGURE_WIDTH = 20\n",
    "PLOTS_PER_ROW = 5\n",
    "\n",
    "def plot_categorical_corr(df):\n",
    "    num_columns = (len(categorical_columns) * (len(categorical_columns) - 1)) // 2\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    plot_idx = 0\n",
    "    for i in range(len(categorical_columns)):\n",
    "        for j in range(i + 1, len(categorical_columns)):\n",
    "            sns.countplot(x=categorical_columns[i], hue=categorical_columns[j], data=data, ax=axes[plot_idx])\n",
    "            axes[plot_idx].set_title(f\"Correlation of {categorical_columns[i]} with {categorical_columns[j]}\")\n",
    "            axes[plot_idx].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "            plot_idx += 1\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(plot_idx, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_numeric_corr(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data[[col for col in numerical_columns if col != 'Price']].corr(), annot=True, cmap='coolwarm', fmt=\".3f\")\n",
    "    plt.title(f\"Numeric Column Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_corr(data)\n",
    "plot_numeric_corr(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845910e8",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57265830",
   "metadata": {},
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(df):\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    for col in numerical_columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "impute_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47304b90",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['IBrand', 'IMaterial', 'ISize', 'Has_Laptop_Compartment', 'Is_Waterproof', 'IStyle', 'IColor']\n",
    "\n",
    "def encode_data(df):\n",
    "    for col in ['Brand', 'Material', 'Style', 'Color']:\n",
    "        df['I' + col] = df[col].astype('category').cat.codes\n",
    "\n",
    "    df['ISize'] = df['Size'].map({'Small': 1, 'Medium': 2, 'Large': 3})\n",
    "\n",
    "    df['Has_Laptop_Compartment'] = df['Laptop Compartment'].map({'Yes': 1, 'No': 0})\n",
    "    df['Is_Waterproof'] = df['Waterproof'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "encode_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a3203",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns += ['Weight_Capacity_Ratio']\n",
    "\n",
    "def normalize_data(df):\n",
    "    df['Weight_Capacity_Ratio'] = df['Weight Capacity (kg)'] / df['Weight Capacity (kg)'].max()\n",
    "\n",
    "normalize_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147cadd",
   "metadata": {},
   "source": [
    "# Target Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = new_columns + ['Compartments']\n",
    "\n",
    "X = data[final_columns]\n",
    "y = data['Price']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b9756",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049ffd3",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "def tune_params(model, param_grid, X, y, cv=10):\n",
    "    cv = KFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(f\"Best parameters:\")\n",
    "    for param_name, param_value in grid_search.best_params_.items():\n",
    "        print(f\"  {param_name}: {param_value}\")\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8ea09",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_regressor(model, X, y, cv=5):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "\n",
    "    mse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    training_times = []\n",
    "    testing_times = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        start_train = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end_train = time.time()\n",
    "        training_times.append(end_train - start_train)\n",
    "\n",
    "        start_test = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        end_test = time.time()\n",
    "        testing_times.append(end_test - start_test)\n",
    "\n",
    "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    print(f\"Avg Mean Squared Error (MSE): {np.mean(mse_scores):.4f}\")\n",
    "    print(f\"Avg Root Mean Squared Error (RMSE): {np.sqrt(np.mean(mse_scores)):.4f}\")\n",
    "    print(f\"Avg Mean Absolute Error (MAE): {np.mean(mae_scores):.4f}\")\n",
    "    print(f\"Avg R² Score: {np.mean(r2_scores):.4f}\")\n",
    "    print(f\"Avg Training Time: {np.mean(training_times):.4f} seconds\")\n",
    "    print(f\"Avg Testing Time: {np.mean(testing_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c770dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_eval(model, param_grid, X, y):\n",
    "    print(f\"Tuning parameters for {model.__class__.__name__}...\")\n",
    "    best_params = tune_params(model, param_grid, X, y)\n",
    "    model.set_params(**best_params)\n",
    "\n",
    "    print()\n",
    "    print(f\"Evaluating {model.__class__.__name__} with best parameters...\")\n",
    "    evaluate_regressor(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74342d",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd90ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=1)\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'friedman_mse'],\n",
    "    'max_depth': [1, 2, 3, 4, 5, None],\n",
    "    'max_features': [1, 2, 3, 5, None],\n",
    "}\n",
    "\n",
    "tune_and_eval(model, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import export_graphviz\n",
    "# from sklearn import tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.tree import plot_tree\n",
    "# plt.figure(figsize=(100, 20))\n",
    "# plot_tree(model, filled=True, feature_names=X.columns, fontsize=10, max_depth=5)\n",
    "# plt.title(\"Decision Tree Visualization\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all the values from hte Brand column\n",
    "# brand_values = train_data['Brand'].unique()\n",
    "# # get average price per brand\n",
    "# avg_price_per_brand = train_data.groupby('Brand')['Price'].mean().sort_values(ascending=False)\n",
    "# avg_price_per_brand\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
