{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9444027e",
   "metadata": {},
   "source": [
    "# üéí Backpack Prediction Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23c3b2",
   "metadata": {},
   "source": [
    "**FEUP 2024/2025 - L.EIC029 IART**\n",
    "\n",
    "- Bruno Oliveira - 202208700  \n",
    "- Henrique Fernandes - 202204988  \n",
    "- Rodrigo Coelho - 202205188  \n",
    "\n",
    "> Based on Kaggle Playground Season 5, Episode 2  \n",
    "> April 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f7e55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc24c1",
   "metadata": {},
   "source": [
    "This project is related to the application of Supervised Learning techniques to real world classification problems. Specifically, we will develop and evaluate machine learning models that accurately predict the price of student backpacks (target variable), based on a variety of input attributes.\n",
    "\n",
    "For this we will use a labeled dataset from Kaggle which contains different bag characteristics and we want to understand how they influence the price.\n",
    "\n",
    "[Link to Backpack Prediction Challenge](https://www.kaggle.com/competitions/playground-series-s5e2/data)\n",
    "\n",
    "We will follow the machine learning pipeline: data preprocessing, problem definition and target identification, model selection and parameter tuning, model training and testing, and result evaluation and comparison.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58250201",
   "metadata": {},
   "source": [
    "### üì¶ Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96d599",
   "metadata": {},
   "source": [
    "In order to setup the project, use the following commands to setup a virtual environment and install the needed dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dfa6d",
   "metadata": {},
   "source": [
    "Once the dependencies are installed, the script below can be used to download the dataset from the Kaggle competition, using your Kaggle account.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Don't forget to download the Kaggle token associated with your account from the <a href=\"https://www.kaggle.com/settings\">Settings page</a>, move it to the current folder and join the <a href=\"https://www.kaggle.com/competitions/playground-series-s5e2/\">Kaggle playground competition</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa775246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "dataset_zip = data_dir.joinpath(\"playground-series-s5e2.zip\")\n",
    "dataset_train = data_dir.joinpath(\"train.csv\")\n",
    "dataset_extra = data_dir.joinpath(\"training_extra.csv\")\n",
    "\n",
    "if not dataset_train.exists() or not dataset_extra.exists():\n",
    "    if not dataset_zip.exists():\n",
    "        print(\"Dataset zip not found. Downloading from Kaggle...\")\n",
    "        !kaggle competitions download -c playground-series-s5e2\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Dataset zip already exists.\")\n",
    "\n",
    "    print(\"Unzipping the dataset...\")\n",
    "    !unzip -o playground-series-s5e2.zip -d data\n",
    "    !rm playground-series-s5e2.zip\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download and extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999020a2",
   "metadata": {},
   "source": [
    "### üìÇ Loading the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de05a04",
   "metadata": {},
   "source": [
    "With the dependencies met and having downloaded the dataset, we can now load it into our environment. The following commands will load the `train.csv` dataset which contains 300000 entries and is used to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(dataset_train)\n",
    "\n",
    "data.info()\n",
    "\n",
    "print(\"\\nFirst 3 rows of the dataset:\")\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce4640",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## üîç Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911decd4",
   "metadata": {},
   "source": [
    "### üåê Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e27fb",
   "metadata": {},
   "source": [
    "As mentioned before, the training dataset consists of *300000* rows of data related to *9* backpack features and the **price** (target variable). \n",
    "\n",
    "Most of these features are **categorical** (brand, material, size, laptop compartment, waterproof, style and color) while the others are **numerical** (compartments and weight capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview_data(data, title):\n",
    "    print(title)\n",
    "    data_shape = data.shape\n",
    "    print(f\"\\nNumber of rows: {data_shape[0]}\")\n",
    "    print(f\"Number of columns: {data_shape[1]}\")\n",
    "    print(\"\\nData types of each column:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "overview_data(data, \"Training Dataset Overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97e267",
   "metadata": {},
   "source": [
    "### ‚ùì Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f33ddc",
   "metadata": {},
   "source": [
    "Although the challenge states that the training dataset was generated from a deep learning model trained on another dataset, most of the features include some missing values which were probably introduced to challenge the data preprocessing.\n",
    "\n",
    "The percentage of **missing values for each feature is below 3.5%** and the **overall percentage of rows with at least one missing value is less than 18%** of the entire training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be864bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pd.DataFrame({\n",
    "    'Column': data.columns,\n",
    "    'Missing Train Values': data.isnull().sum().values,\n",
    "    'Percentage of Missing Train Values': data.isnull().sum().values / len(data) * 100\n",
    "})\n",
    "\n",
    "missing_values = missing_values[~missing_values['Column'].isin(['id', 'Price'])]\n",
    "\n",
    "summary_row = pd.DataFrame([{\n",
    "    'Column': 'Rows with Missing Values',\n",
    "    'Missing Train Values': data.isnull().any(axis=1).sum(),\n",
    "    'Percentage of Missing Train Values': data.isnull().any(axis=1).sum() / len(data) * 100\n",
    "}])\n",
    "\n",
    "missing_values = pd.concat([missing_values, summary_row], ignore_index=True)\n",
    "\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259fcdd",
   "metadata": {},
   "source": [
    "### üß¨ Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d661d",
   "metadata": {},
   "source": [
    "Despite having a moderate amount of missing data, there are no duplicated rows in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_duplicates = data.drop('id', axis=1).duplicated().sum()\n",
    "print(f\"Data duplicates: {data_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddadd5",
   "metadata": {},
   "source": [
    "### üßÆ Distribution of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0ff56",
   "metadata": {},
   "source": [
    "When it comes to data distribution, all columns follow a relatively constant distribution without any specific outliers or imbalances. Although every categorical feature has a (slightly) dominant category, there are no significant differences in terms of the frequencies of each value. This is also the case for the numeric columns where the distribution of the value count is approximately constant.\n",
    "\n",
    "The graphs don't show any major skewness meaning the dataset appears to be well-suited for classification of all types of backpacks without a specific bias from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "categorical_columns = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "numerical_columns = ['Compartments', 'Weight Capacity (kg)', 'Price']\n",
    "\n",
    "FIGURE_WIDTH = 18\n",
    "PLOTS_PER_ROW = 3\n",
    "\n",
    "def plot_categorical_columns(df):\n",
    "    num_columns = len(categorical_columns)\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(categorical_columns):\n",
    "        sns.countplot(data=df, x=column, order=df[column].value_counts().index, ax=axes[i])\n",
    "        axes[i].set_title(f\"Distribution of {column}\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_numeric_columns(df):\n",
    "    num_columns = len(numerical_columns)\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(numerical_columns):\n",
    "        sns.histplot(df[column], ax=axes[i], discrete=column == 'Compartments')\n",
    "        axes[i].set_title(f\"Distribution of {column}\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_columns(data)\n",
    "plot_numeric_columns(data)\n",
    "\n",
    "data.drop('id', axis=1).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4893593",
   "metadata": {},
   "source": [
    "### üîó Data Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af21830",
   "metadata": {},
   "source": [
    "The correlation analysis revealed that no strong linear relationships appear to exist between the columns that could influence the target variable and, consequently, the model training.\n",
    "\n",
    "As it can be seen both from the Categorical vs. Categorical countplots, there are not major correlations between different categrical features of the dataset. A similar conclusion can be taken from the correlation heatmap of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5082b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_WIDTH = 20\n",
    "PLOTS_PER_ROW = 3\n",
    "\n",
    "def plot_categorical_corr(df):\n",
    "    num_columns = (len(categorical_columns) * (len(categorical_columns) - 1)) // 2\n",
    "    num_rows = math.ceil(num_columns / PLOTS_PER_ROW)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, PLOTS_PER_ROW, figsize=(FIGURE_WIDTH, FIGURE_WIDTH / PLOTS_PER_ROW * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_idx = 0\n",
    "    for i in range(len(categorical_columns)):\n",
    "        for j in range(i + 1, len(categorical_columns)):\n",
    "            sns.countplot(x=categorical_columns[i], hue=categorical_columns[j], data=data, ax=axes[plot_idx])\n",
    "            axes[plot_idx].set_title(f\"Correlation of {categorical_columns[i]} with {categorical_columns[j]}\")\n",
    "            axes[plot_idx].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "            plot_idx += 1\n",
    "\n",
    "    for j in range(plot_idx, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_numeric_corr(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(data[[col for col in numerical_columns if col != 'Price']].corr(), annot=True, cmap='coolwarm', fmt=\".3f\", cbar=True)\n",
    "    plt.title(f\"Numeric Column Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "plot_categorical_corr(data)\n",
    "plot_numeric_corr(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845910e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57265830",
   "metadata": {},
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(df):\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    for col in numerical_columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "impute_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47304b90",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['IBrand', 'IMaterial', 'ISize', 'Has_Laptop_Compartment', 'Is_Waterproof', 'IStyle', 'IColor']\n",
    "\n",
    "def encode_data(df):\n",
    "    for col in ['Brand', 'Material', 'Style', 'Color']:\n",
    "        df['I' + col] = df[col].astype('category').cat.codes\n",
    "\n",
    "    df['ISize'] = df['Size'].map({'Small': 1, 'Medium': 2, 'Large': 3})\n",
    "\n",
    "    df['Has_Laptop_Compartment'] = df['Laptop Compartment'].map({'Yes': 1, 'No': 0})\n",
    "    df['Is_Waterproof'] = df['Waterproof'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "encode_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a3203",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns += ['Weight_Capacity_Ratio']\n",
    "\n",
    "def normalize_data(df):\n",
    "    df['Weight_Capacity_Ratio'] = df['Weight Capacity (kg)'] / df['Weight Capacity (kg)'].max()\n",
    "\n",
    "normalize_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147cadd",
   "metadata": {},
   "source": [
    "### Target Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9db1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = new_columns + ['Compartments']\n",
    "\n",
    "X = data[final_columns]\n",
    "y = data['Price']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b9756",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049ffd3",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "def tune_params(model, param_grid, X, y, cv=10):\n",
    "    cv = KFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(f\"Best parameters:\")\n",
    "    for param_name, param_value in grid_search.best_params_.items():\n",
    "        print(f\"  {param_name}: {param_value}\")\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8ea09",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_regressor(model, X, y, cv=5):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=1)\n",
    "\n",
    "    mse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    training_times = []\n",
    "    testing_times = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        start_train = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end_train = time.time()\n",
    "        training_times.append(end_train - start_train)\n",
    "\n",
    "        start_test = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        end_test = time.time()\n",
    "        testing_times.append(end_test - start_test)\n",
    "\n",
    "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    print(f\"Avg Mean Squared Error (MSE): {np.mean(mse_scores):.4f}\")\n",
    "    print(f\"Avg Root Mean Squared Error (RMSE): {np.sqrt(np.mean(mse_scores)):.4f}\")\n",
    "    print(f\"Avg Mean Absolute Error (MAE): {np.mean(mae_scores):.4f}\")\n",
    "    print(f\"Avg R¬≤ Score: {np.mean(r2_scores):.4f}\")\n",
    "    print(f\"Avg Training Time: {np.mean(training_times):.4f} seconds\")\n",
    "    print(f\"Avg Testing Time: {np.mean(testing_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c770dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_eval(model, param_grid, X, y):\n",
    "    print(f\"Tuning parameters for {model.__class__.__name__}...\")\n",
    "    best_params = tune_params(model, param_grid, X, y)\n",
    "    model.set_params(**best_params)\n",
    "\n",
    "    print()\n",
    "    print(f\"Evaluating {model.__class__.__name__} with best parameters...\")\n",
    "    evaluate_regressor(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74342d",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd90ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=1)\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'friedman_mse'],\n",
    "    'max_depth': [1, 2, 3, 4, 5, None],\n",
    "    'max_features': [1, 2, 3, 5, None],\n",
    "}\n",
    "\n",
    "tune_and_eval(model, param_grid, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import export_graphviz\n",
    "# from sklearn import tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.tree import plot_tree\n",
    "# plt.figure(figsize=(100, 20))\n",
    "# plot_tree(model, filled=True, feature_names=X.columns, fontsize=10, max_depth=5)\n",
    "# plt.title(\"Decision Tree Visualization\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all the values from hte Brand column\n",
    "# brand_values = train_data['Brand'].unique()\n",
    "# # get average price per brand\n",
    "# avg_price_per_brand = train_data.groupby('Brand')['Price'].mean().sort_values(ascending=False)\n",
    "# avg_price_per_brand\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
